{"meta":{"title":"滑天下之大稽","subtitle":"无问西东","description":"","author":"LogicJake","url":"https://www.logicjake.xyz","root":"/"},"pages":[{"title":"404","date":"2021-07-03T16:10:11.497Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"404.html","permalink":"https://www.logicjake.xyz/404.html","excerpt":"","text":"404 true"},{"title":"About Me","date":"2019-12-28T13:33:31.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"about/index.html","permalink":"https://www.logicjake.xyz/about/index.html","excerpt":"","text":"教育南京航空航天大学 2019.9 - 专业硕士 计算机技术 研究方向：复杂网络表示 南京航空航天大学 2015.9 - 2019.6 本科 物联网工程 竞赛经历“添翼”杯人工智能创新应用大赛-智慧教育赛道 2019.7 - 2019.9复赛第三，决赛三等奖 科大讯飞移动广告反欺诈算法挑战赛 2019.7 - 2019.9复赛第二，决赛亚军 国能日新第二届光伏功率预测赛 2019.8 -2019.106 / 182 Kaggle: 2019 Data Science Bowl 2019.12 - 2020.1Top 9%，铜牌 图灵联邦视频点击预测大赛 2019.11 - 2020.2第三 高能对撞粒子分类挑战赛 2019.12 - 2020.311 / 256 天池2020数字中国创新大赛—算法赛：智慧海洋建设 2019.12 - 2020.355 / 3275 天池二手车交易价格预测 2019.3 -2020.462 / 2776 腾讯2020游戏安全技术竞赛机器学习赛道 2020.4 -2020.4第四 Kaggle: University of Liverpool - Ion Switching 2020.3 -2020.5Top 2%，银牌 KDD Cup 2020 Challenges for Modern E-Commerce Platform: Debiasing 2020.4 -2020.613 / 1895 2020腾讯广告算法大赛 2020.5 -2020.812 / 10000 第二届翼支付杯大数据建模大赛-信用风险用户识别 2020.7 -2020.9复赛第二"},{"title":"categories","date":"2019-12-28T12:24:38.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"categories/index.html","permalink":"https://www.logicjake.xyz/categories/index.html","excerpt":"","text":""},{"title":"","date":"2021-07-03T16:10:11.497Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"links/index.html","permalink":"https://www.logicjake.xyz/links/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-12-28T12:24:21.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"tags/index.html","permalink":"https://www.logicjake.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"KDD Cup 2020 Challenges for Modern E-Commerce Platform: Debiasing TOP13赛后总结","slug":"KDD-debias-TOP13","date":"2020-06-16T14:18:17.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"2020/06/16/KDD-debias-TOP13/","link":"","permalink":"https://www.logicjake.xyz/2020/06/16/KDD-debias-TOP13/","excerpt":"","text":"最终成绩：Full榜15，Half榜13比赛地址：https://tianchi.aliyun.com/competition/entrance/231785/源码：https://github.com/LogicJake/2020_KDD_Debiasing_TOP13 赛题背景该挑战着重于曝光的公平性，即推荐过去很少曝光的项目，以对抗在推荐系统中经常遇到的 Matthew 效应。特别是，在对点击数据进行训练时执行偏差减少对该任务的成功至关重要。就像现代推荐系统中记录的点击数据与实际的在线环境有差距一样，训练数据与测试数据也会有差距，主要体现在趋势和物品的受欢迎程度上。获胜的解决方案需要在历史上很少暴露的项目上表现良好。训练数据和测试数据是在许多时期收集的，其中包括大规模的销售活动。由于趋势的变化，为了进行可靠的预测，减少偏差是不可避免的。我们提供物品的多模式特性以及一些(匿名的)关键用户特性，以帮助参与者探索解决方案，以调整数据偏差，并能够很好地处理未探索过的物品。 建模比赛给出的数据如下所示，××_click_× 是用户在各个阶段的点击记录，××_qtime_× 是需要预测的用户相对的最后一次点击。数据按划窗的方式逐阶段（phase）给出，各阶段的时间分布如图1所示。各阶段大概有4天的数据，相邻阶段数据时间有交叉。且单阶段中，训练集用户和测试集用户不交叉。原始数据集只给出了测试集的 qtime ，为了获取稳定的线下验证，需要我们自己构造有标签的训练集 qtime。基于题设，我们抽出每个阶段训练集用户的最后一条 click 数据作为训练集 qtime 数据，用于线下验证，形式化表示如图2所示。 123456789101112131415|-- data |-- underexpose_train |-- underexpose_user_feat.csv |-- underexpose_item_feat.csv |-- underexpose_train_click-0.csv |-- ... |-- underexpose_train_click-9.csv |-- underexpose_test |-- underexpose_test_click-0 |-- underexpose_test_qtime-0.csv |-- underexpose_test_click-0.csv |-- ... |-- underexpose_test_click-9 |-- underexpose_test_qtime-9.csv |-- underexpose_test_click-9.csv 图1 各阶段时间分布 图2 训练集 qtime 构造方式 问题被切分为召回和排序两个部分，总共六路召回，最后合并删除重复召回的商品。排序阶段建模为二分类问题，采用了两个深度模型和一个LGB模型。整体流程如图3所示。 图3 整体流程图 召回在召回阶段，我们采用了6路召回，通过不同策略的差异性提高整体商品的召回率。这6种方法或利用点击序列信息提高了热门商品的召回率，或采用商品的属性信息提高冷门商品的召回率，或二者兼顾。在所有召回策略中，均是分 phase 召回。 改进版 itemcf原始的 itemcf 将用户点击过的商品看做一个无序的集合，但在实际应用中，应该考虑到点击次序和时间带来的影响。在计算同一序列中两个商品的相似度时，不仅需要考虑其共现次数，也需要考虑两个商品之间的次序关系和时间关系。同一点击序列中两个商品位置越远或者点击时间间隔越大，相关性应该减小。正因为存在时间和距离衰减，我们跳过了点击距离过远（大于5）或点击时间过长（大于0.000003）的商品对的相似度计算。商品对顺序和逆序的权重也不同，在点击序列A，B，C中，”BC”这样的正序权重应该大于”BA”这样的逆序权重。为了解决偏差问题，除了从序列上建立相似性关系，也可以将商品之间的文本相似度和图像相似度纳入相似性因素，引入这二者可以极大地提高冷门商品的召回率。 建立商品的相似度关系后，进入到给用户召回商品阶段，根据用户的交互商品，结合商品相似度选择 TOP100 关联商品。选取关联商品时，除了考虑和历史交互商品的相似度，还要加入时间衰减，位置距离衰减和候选商品热度。 我们总共有两版 itemcf，二者或在上述策略的部分选用上有所差异或者衰减函数有所差异。我们保存了本阶段得到的商品相似度，供后续特征工程进一步使用。 item2vec除了使用人工规则从序列中提取相似度，我们还可以使用序列学习模型 Word2Vec 为商品学习向量表示。将用户的商品点击序列作为句子喂到 Word2Vec 模型，然后选取和用户最近交互商品最相似的关联商品。向量学习和寻找相似全部使用 gensim 库的 Word2Vec 实现，向量维度和迭代次数对最终结果影响较大。该方法能够极好地召回冷门商品。 1234567# 模型训练model &#x3D; Word2Vec(sentences, size&#x3D;256, window&#x3D;5, min_count&#x3D;1, sg&#x3D;1, hs&#x3D;0, seed&#x3D;seed, iter&#x3D;300, negative&#x3D;5, workers&#x3D;6)# 寻找关联商品interacted_items &#x3D; user_item[user_id]sim_items &#x3D; model.wv.most_similar(positive&#x3D;[str(x) for x in interacted_items[-2:]], topn&#x3D;100) 基于商品属性的 ANN 召回基于交互序列的 itemcf 倾向于召回热门商品，为了进一步提高冷门商品的召回，我们采用了基于商品属性信息的 ANN 召回方法。数据集本身给出了商品的文本表示和图像表示，所以可以分别针对商品的文本 embedding 和图像 embedding 进行向量召回。我们对用户历史交互商品的向量表示进行加权平均得到用户的表示向量。在相似度计算召回阶段，如果使用矩阵运算空间复杂度会非常高，如果采用双层 for 循环则时间复杂度又太高，所以我们使用 Annoy 工具。Annoy 是 Spotify 开源的一个用于近似最近邻查询的 C++/Python 工具，对内存使用进行了优化，索引可以在硬盘保存或者加载。简单来说，先将商品的向量表示建立索引，然后用用户的向量表示寻找近似的最相似商品。Annoy 稍微牺牲了相似度计算的准确度，却极大地提高了查找速度。 对比基于商品文本信息和图像信息的召回，基于文本的召回方式要胜于基于图像的召回方式，可能是因为文本能更好地描述商品，而图片信息噪声相对较大。 基于网络的召回该方法源自”Bipartite network projection and personal recommendation”，代码采用自论坛开源。该方法也分为两个阶段，商品相似度计算和基于用户交互历史的商品召回。在相似度计算阶段，通过用户将两个商品连接起来。计算相似度时考虑两种因素：1)两个商品的共同被点击用户过多，则相似度减少；2)共同被点击用户的交互商品过多，相似度也要减少。基于用户交互历史的商品召回和 itemcf 类似，不再赘述。 我们设置每路都召回100个商品，但这之间肯定存在重复召回的情况，这也是为什么召回策略讲究差异性，其实就是为了减少重复召回的数量。去重之后，平均每个用户召回到440个商品。因为每个用户在 qtime 中只会点击一个商品，所以召回构造的样本正负比也会比较大，所以进一步地我们删除了召回未命中的用户样本，减少了无用负样本的数量，也提高了后续排序模型的效果。 假设训练集用户A 在阶段0的 qtime 真实点击了商品i，基于用户A的阶段0历史交互记录，我们为其召回到商品i，j，k，l，那么在召回阶段结束我们可以得到如下样本。| user_id | phase | query_time | item_id | label || :—–: | :—: | :———-: | :—–: | :—: || A | 0 | 0.9839420823 | i | 1 || A | 0 | 0.9839420823 | j | 0 || A | 0 | 0.9839420823 | k | 0 || A | 0 | 0.9839420823 | l | 0 | 排序我们把排序建模为二分类任务，分为特征工程和二分类模型预测两部分。特征工程主要围绕召回策略进行，大量使用用户历史交互商品和待预测商品的相似度特征。模型包括两个基于 LSTM 的深度模型和一个 LGB 模型，两个深度模型的预测概率作为 LGB 模型的特征。 特征工程特征工程分为三大类：商品属性，用户属性，用户-商品交互属性。数据集本身给出的属性信息较少，所以特征工程主要围绕交互属性展开。商品信息给出了128维的图像向量和128维的文本向量，原始向量维度较大，所以我们分别对其使用 PCA 降维，降维后反而提高了模型效果。除此以外统计了商品被点击次数，平均点击间隔，点击用户的年龄统计和性别统计。 用户特征包括： user_age_level：用户所属的年龄段（原始属性） user_gender：用户的性别，可以为空（原始属性） user_city_level：用户所在城市的等级（原始属性） 历史点击商品次数 历史点击时间的统计特征（min,max,std） 预测时间点距离用户最进一次点击的时间差 用户历史点击时间跨度（max-min） 交互特征主要基于之前的召回策略进行，通过保存召回阶段的商品相似度信息或向量，我们能够间接或直接得到用户对待预测商品的评分。基于商品属性的ANN召回已经得到用户和商品的向量表示，通过余弦相似度可以直接得到用户对待预测商品的评分。基于 itemcf， Bipartite network 和 item2vec 的召回得到的只是商品之间的相似度，需要和用户的历史交互商品计算间接得到用户-商品评分，采用如下方式： 待预测商品和用户所有历史交互商品最大相似度 待预测商品和用户所有历史交互商品相似度按次序加权求和 待预测商品和用户最近一次交互商品相似度 待预测商品和用户最近k次交互商品相似度之和 待预测商品和用户最近k次交互商品相似度平均 模型两个深度模型的结构分别如图4和图5所示，LSTM 能够很好地学习用户交互序列，所以两个模型都用到了 LSTM 层。图4的深度模型除了使用 LSTM 学习序列特征，还使用了 Attention 层进一步捕捉序列关系。除了学习序列特征，其还使用了特征工程部分输出的特征，将两类特征 concat 送入两层全连接层。 图4 深度模型1结构图 图5 深度模型2结构图 图5模型与图4的主要不同点在于，将阶段 0-6 的序列输入到 Word2Vec 模型，预训练得到商品的 embedding，然后冻结模型中的 embedding layer。 特征工程得到的特征加上两个深度模型的预测概率特征输入到 LGB 模型，输出每个用户点击某商品的最终概率。 后处理我们进一步对预测概率值进行后处理，减轻商品推荐的偏差问题。统计每个商品历史被点击次数，适当让点击次数较少的商品被推荐概率放大。最开始我们让概率除以商品点击次数，但这种方式较为极端，极大提高 half 指标的同时，full 指标下降严重。因为复赛要保证 full 指标在前65成绩才有效，为了稳妥起见，我们选取了另外一种后处理方式：概率除以商品点击次数的开方，提高 half 的同时平衡 full 指标的下降。 参考资料改进青禹小生baseline，phase3线上：0.2A simple itemCF Baseline, score:0.1169(phase0-2) A Simple Recall Method based on Network-based Inference，score:0.18 (phase0-3)天池-安泰杯跨境电商智能算法大赛分享（冠军）","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.logicjake.xyz/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"召回","slug":"召回","permalink":"https://www.logicjake.xyz/tags/%E5%8F%AC%E5%9B%9E/"},{"name":"排序","slug":"排序","permalink":"https://www.logicjake.xyz/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"天池二手车交易价格预测-赛后总结","slug":"天池二手车交易价格预测-赛后总结","date":"2020-04-21T20:36:11.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"2020/04/21/天池二手车交易价格预测-赛后总结/","link":"","permalink":"https://www.logicjake.xyz/2020/04/21/%E5%A4%A9%E6%B1%A0%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BA%A4%E6%98%93%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B-%E8%B5%9B%E5%90%8E%E6%80%BB%E7%BB%93/","excerpt":"","text":"最终成绩：62 / 2776比赛地址：https://tianchi.aliyun.com/competition/entrance/231784/introduction源码：https://github.com/LogicJake/competition_baselines/tree/master/competitions/tianchi_car_sale/final 比赛介绍本次新人赛是Datawhale与天池联合发起的0基础入门系列赛事第一场 —— 零基础入门数据挖掘之二手车交易价格预测大赛。 赛题以二手车市场为背景，要求选手预测二手汽车的交易价格，这是一个典型的回归问题。通过这道赛题来引导大家走进AI数据竞赛的世界，主要针对于于竞赛新人进行自我练习、自我提高。 为了更好的引导大家入门，我们同时为本赛题定制了系列学习方案，其中包括数据科学库、通用流程和baseline方案学习三部分。通过对本方案的完整学习，可以帮助掌握数据竞赛基本技能。同时我们也将提供专属的视频直播学习通道。 EDA详细的 eda 可以参考天才儿童在天池的分享: https://tianchi.aliyun.com/notebook-ai/detail?postId=95276 数据集的格式如下： 数据格式 特征可以分成三类: 日期特征: regDate, creatDate 类别特征: name, model, brand, bodyType, fuelType, gearbox, notRepairedDamage, regionCode, seller, offerType 数值特征: power, kilometer和15个匿名特征 这里主要关注特征的缺失率和 nunique 信息，主要是看有没有缺失过多或 nunique 太少的特征，一般情况下这两种特征对模型学习起不到作用。数值特征 power 和 kilometer nunique 值比较少，也不知道是不是数据做了处理，抹去了精度。seller 和 offerType 只有两个甚至1个不同的值，所以可以删去, 对模型学习起不到作用，模型的特征重要性也为0。 数据统计 匿名特征的分布见下图，匿名特征在最后的模型重要性都挺高的，可以好好挖掘一下。 匿名特征 数据处理缺失值处理缺失值主要集中在bodyType，fuelType，gearbox，我的思路是汽车的指标往往和其所属的品牌和车型有较大关系，所以采用该品牌车型下的众数来填补缺失值。 12345678910111213141516171819202122from scipy import statscols &#x3D; [&#39;bodyType&#39;, &#39;fuelType&#39;, &#39;gearbox&#39;]df_feature[&#39;gp&#39;] &#x3D; df_feature[&#39;brand&#39;].astype( &#39;str&#39;) + df_feature[&#39;model&#39;].astype(&#39;str&#39;)gp_col &#x3D; &#39;gp&#39;df_na &#x3D; df_feature[cols].isna()df_mode &#x3D; df_feature.groupby(gp_col)[cols].agg( lambda x: stats.mode(x)[0][0])for col in cols: na_series &#x3D; df_na[col] names &#x3D; list(df_feature.loc[na_series, gp_col]) t &#x3D; df_mode.loc[names, col] t.index &#x3D; df_feature.loc[na_series, col].index df_feature.loc[na_series, col] &#x3D; tdel df_feature[&#39;gp&#39;]df_feature[cols].isnull().sum() 目标变量分布变换一般来说对于回归问题，目标变量正态化对模型预测有帮助，下图展示了使用 log1p 前后的价格分布情况。 价格分布 无效特征删除seller 和 offerType 只有两个甚至1个不同的值，所以可以删去, 对模型学习起不到作用，模型的特征重要性也为0。 特征工程基础特征对于两个日期特征汽车注册日期和开始售卖时间，可以二者做差值计算汽车售卖时的使用时间，我这里使用了年和天来刻画。除此以外，汽车是哪一年注册的对价格的影响也挺大。数据中存在一些异常日期数据：月份为0，处理的时候将其置为1即可。 12df_feature[&#39;car_age_day&#39;] &#x3D; ( df_feature[&#39;creatDate&#39;] - df_feature[&#39;regDate&#39;]).dt.daysdf_feature[&#39;car_age_year&#39;] &#x3D; round(df_feature[&#39;car_age_day&#39;] &#x2F; 365, 1) 对于类别特征, 可以计算count属性, 反应销售热度。 1df_feature[&#39;name_count&#39;] &#x3D; df_feature.groupby([&#39;name&#39;])[&#39;SaleID&#39;].transform(&#39;count&#39;) 数值特征往往结合类别特征进行统计。比如可以统计不同汽车品牌下匿名特征的统计特征：mean, std, max, min。 12345l &#x3D; [&#39;name&#39;, &#39;model&#39;, &#39;brand&#39;, &#39;bodyType&#39;]for f1 in tqdm(l): for f2 in v_cols: df_feature &#x3D; stat(df_feature, df_feature, [f1], &#123; f2: [&#39;mean&#39;, &#39;max&#39;, &#39;min&#39;, &#39;std&#39;]&#125;) 目标变量 price 也是数值特征，所以也可以结合类别进行统计，比如计算某品牌，某车型的平均交易价格，这种做法称为目标编码。但需要注意的是，假如使用全局标签信息统计会出现标签泄露的问题，所以一般使用五折统计法，用四折的标签数据做统计给另外一折的数据做特征。 匿名特征简单一点，可以直接统计每辆车15个匿名特征的统计值，得到v_mean，v_max，v_min和v_std。然后再统计汽车交易名称下这四个特征的统计值，这道题，汽车交易名称也是一个很重要的特征。 12345678910111213141516v_cols &#x3D; [&#39;v_&#39;+str(i) for i in range(15)]df_feature[&#39;v_mean&#39;] &#x3D; df_feature[v_cols].mean(axis&#x3D;1)df_feature[&#39;v_max&#39;] &#x3D; df_feature[v_cols].max(axis&#x3D;1)df_feature[&#39;v_min&#39;] &#x3D; df_feature[v_cols].min(axis&#x3D;1)df_feature[&#39;v_std&#39;] &#x3D; df_feature[v_cols].std(axis&#x3D;1)for col in [&#39;v_mean&#39;, &#39;v_max&#39;, &#39;v_min&#39;, &#39;v_std&#39;]: df_feature[f&#39;name_&#123;col&#125;_mean&#39;] &#x3D; df_feature.groupby(&#39;name&#39;)[ col].transform(&#39;mean&#39;) df_feature[f&#39;name_&#123;col&#125;_std&#39;] &#x3D; df_feature.groupby(&#39;name&#39;)[ col].transform(&#39;std&#39;) df_feature[f&#39;name_&#123;col&#125;_max&#39;] &#x3D; df_feature.groupby(&#39;name&#39;)[ col].transform(&#39;max&#39;) df_feature[f&#39;name_&#123;col&#125;_min&#39;] &#x3D; df_feature.groupby(&#39;name&#39;)[ col].transform(&#39;min&#39;) 匿名特征无法知道具体的业务含义，所以只能梭哈操作，写了个程序对匿名特征进行二阶或三阶组合，计算相加和相减，最后筛选保留以下特征： 123456789101112131415161718df_feature[&#39;v_0_add_v_4&#39;] &#x3D; df_feature[&#39;v_0&#39;] + df_feature[&#39;v_4&#39;]df_feature[&#39;v_0_add_v_8&#39;] &#x3D; df_feature[&#39;v_0&#39;] + df_feature[&#39;v_8&#39;]df_feature[&#39;v_1_add_v_3&#39;] &#x3D; df_feature[&#39;v_1&#39;] + df_feature[&#39;v_3&#39;]df_feature[&#39;v_1_add_v_4&#39;] &#x3D; df_feature[&#39;v_1&#39;] + df_feature[&#39;v_4&#39;]df_feature[&#39;v_1_add_v_5&#39;] &#x3D; df_feature[&#39;v_1&#39;] + df_feature[&#39;v_5&#39;]df_feature[&#39;v_1_add_v_12&#39;] &#x3D; df_feature[&#39;v_1&#39;] + df_feature[&#39;v_12&#39;]df_feature[&#39;v_2_add_v_3&#39;] &#x3D; df_feature[&#39;v_2&#39;] + df_feature[&#39;v_3&#39;]df_feature[&#39;v_4_add_v_11&#39;] &#x3D; df_feature[&#39;v_4&#39;] + df_feature[&#39;v_11&#39;]df_feature[&#39;v_4_add_v_12&#39;] &#x3D; df_feature[&#39;v_4&#39;] + df_feature[&#39;v_12&#39;]df_feature[&#39;v_0_add_v_12_add_v_14&#39;] &#x3D; df_feature[&#39;v_0&#39;] + \\ df_feature[&#39;v_12&#39;] + df_feature[&#39;v_14&#39;]df_feature[&#39;v_4_add_v_9_minu_v_13&#39;] &#x3D; df_feature[&#39;v_4&#39;] + \\ df_feature[&#39;v_9&#39;] - df_feature[&#39;v_13&#39;]df_feature[&#39;v_2_add_v_4_minu_v_11&#39;] &#x3D; df_feature[&#39;v_2&#39;] + \\ df_feature[&#39;v_4&#39;] - df_feature[&#39;v_11&#39;]df_feature[&#39;v_2_add_v_3_minu_v_11&#39;] &#x3D; df_feature[&#39;v_2&#39;] + \\ df_feature[&#39;v_3&#39;] - df_feature[&#39;v_11&#39;] 尝试过的无用特征论坛分享了不少结合业务的特征，但我自己尝试后发现效果都不行，这也是让我很困惑的地方。官方赛题分享提到可以截取regionCode，提取城市信息，但 regionCode 已经被编码脱敏成0~8121的数字，已经无法进行信息提取。 模型两个树模型：lgb 和 xgb 分别预测，然后根据得分进行简单的加权，按照 0.45*xgb_pred+0.55 *lgb_pred 得到最后的汽车预测价格。最后线上得分433，rank：62 / 2776。","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"表格数据","slug":"表格数据","permalink":"https://www.logicjake.xyz/tags/%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE/"},{"name":"价格预测","slug":"价格预测","permalink":"https://www.logicjake.xyz/tags/%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"}]},{"title":"2020腾讯游戏安全技术竞赛机器学习组-TOP 4赛后总结","slug":"2020腾讯游戏安全技术竞赛机器学习组-赛后总结","date":"2020-04-16T18:38:56.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"2020/04/16/2020腾讯游戏安全技术竞赛机器学习组-赛后总结/","link":"","permalink":"https://www.logicjake.xyz/2020/04/16/2020%E8%85%BE%E8%AE%AF%E6%B8%B8%E6%88%8F%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF%E7%AB%9E%E8%B5%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%84-%E8%B5%9B%E5%90%8E%E6%80%BB%E7%BB%93/","excerpt":"最终成绩：优胜奖（TOP 4）比赛地址：https://gslab.qq.com/html/competition/2020/index.htm源码：https://github.com/LogicJake/2020-gslab-ml-top4 赛题背景在游戏中，有一些人或组织，通过非法手段获取大量游戏内金币物品等资源，这就是打金工作室（Gold Farming）。他们一般拥有大量帐号，使用外挂或挂机软件批量进行游戏行为，破坏游戏经济系统，影响玩家游戏体验。可自行搜索打金工作室相关资料，了解他们是如何运作，游戏方如何打击，他们又是如何逃避打击的。 竞赛目标使用 2020年03月01日的数据(含标签) 来训练识别打金工作室的模型。用此模型, 预测2020年03月05日的打金工作室。取当日login或logout中出现过的帐号，判断这些帐号中哪些是打金工作室。 衡量标准得分 = 4PR / (P+3R) P为准确度, 即提交结果有多大比率是真正的工作室R为覆盖率, 即提交结果覆盖了全体工作室的比率","text":"最终成绩：优胜奖（TOP 4）比赛地址：https://gslab.qq.com/html/competition/2020/index.htm源码：https://github.com/LogicJake/2020-gslab-ml-top4 赛题背景在游戏中，有一些人或组织，通过非法手段获取大量游戏内金币物品等资源，这就是打金工作室（Gold Farming）。他们一般拥有大量帐号，使用外挂或挂机软件批量进行游戏行为，破坏游戏经济系统，影响玩家游戏体验。可自行搜索打金工作室相关资料，了解他们是如何运作，游戏方如何打击，他们又是如何逃避打击的。 竞赛目标使用 2020年03月01日的数据(含标签) 来训练识别打金工作室的模型。用此模型, 预测2020年03月05日的打金工作室。取当日login或logout中出现过的帐号，判断这些帐号中哪些是打金工作室。 衡量标准得分 = 4PR / (P+3R) P为准确度, 即提交结果有多大比率是真正的工作室R为覆盖率, 即提交结果覆盖了全体工作室的比率 数据说明数据来自某MMORPG(大型多人在线角色扮演游戏), 并经过脱敏处理 日期 2020年03月01日的数据(含标签) 为训练集 2020年03月05日的数据为测试集 基础知识 uin: 唯一标识游戏内的一个用户, 比如你的qq或微信 roleid: 一个uin可能有多个角色 存储格式 文件名为: 年月日.txt 以文本存储 以竖线|分隔 空 或 \\N 表示数据缺失 目录说明 label_black 黑标签: 打金工作室帐号 label_white 白标签: 非打金工作室帐号 role_login 角色登入游戏 role_logout 角色登出游戏 role_create 创建新角色 uin_chat 按天统计的帐号发言次数 以下数据仅在决赛时提供 role_moneyflow 角色的详细金钱流水信息(当天按时间顺序前300条记录) role_itemflow 角色的详细物品流水信息(当天按时间顺序前300条记录) role_login 角色登入游戏 # 列名 类型 备注 1 dteventtime STRING 时间,格式YYYY-MM-DD HH:MM:SS 2 platid BIGINT ios=0/android=1 3 areaid BIGINT 微信=1/手Q=2/游客=3 4 worldid BIGINT 游戏小区(已加密) 5 uin STRING openid(已加密) 6 roleid STRING 角色id(已加密) 7 rolename STRING 角色名(已置空) 8 job STRING 职业 9 rolelevel BIGINT 等级 10 power BIGINT 战力 11 friendsnum BIGINT 好友数量 12 network STRING 3G/WIFI/2G/NULL 13 clientip STRING 客户端IP(已加密) 14 deviceid STRING 设备ID(已加密) role_logout 角色登出游戏 # 列名 类型 备注 1 dteventtime STRING 时间,格式YYYY-MM-DD HH:MM:SS 2 platid BIGINT ios=0/android=1 3 areaid BIGINT 微信=1/手Q=2/游客=3 4 worldid BIGINT 游戏小区(已加密) 5 uin STRING openid(已加密) 6 roleid STRING 角色id(已加密) 7 rolename STRING 角色名(已置空) 8 job STRING 职业 9 rolelevel BIGINT 等级 10 power BIGINT 战力 11 friendsnum BIGINT 好友数量 12 network STRING 3G/WIFI/2G/NULL 13 clientip STRING 客户端IP(已加密) 14 deviceid STRING 设备ID(已加密) 15 onlinetime BIGINT 在线时长(秒) role_create 创建新角色 # 列名 类型 备注 1 dteventtime STRING YYYY-MM-DD HH#MM#SS 2 platid BIGINT ios=0/android=1 3 areaid BIGINT 微信=1/手Q=2/游客=3 4 worldid BIGINT 游戏小区(已加密) 5 uin STRING openid(已加密) 6 roleid STRING 角色id(已加密) 7 rolename STRING 角色名(已置空) 8 job STRING 职业 9 regchannel STRING 注册渠道 10 network STRING 3G/WIFI/2G 11 clientip STRING 客户端IP(已加密) 12 deviceid STRING 设备ID(已加密) uin_chat 按天统计的帐号发言次数 # 列名 类型 备注 1 uin STRING openid(已加密) 2 chat_cnt BIGINT 发言条数 role_moneyflow 帐号的详细金钱流水信息 # 列名 类型 备注 1 dteventtime STRING 时间,格式YYYY-MM-DD HH:MM:SS 2 worldid BIGINT 游戏小区(已加密) 3 uin STRING openid(已加密) 4 roleid STRING 角色id(已加密) 5 rolelevel BIGINT 等级 6 iMoneyType STRING 货币类型 7 iMoney BIGINT 货币变化数 8 AfterMoney BIGINT 动作后的货币存量 9 AddOrReduce BIGINT 货币增加 0/减少 1 10 Reason STRING 货币流动一级原因 11 SubReason STRING 货币流动二级原因 role_itemflow 帐号的详细物品流水信息 # 列名 类型 备注 1 dteventtime STRING 时间,格式YYYY-MM-DD HH:MM:SS 2 worldid BIGINT 游戏小区(已加密) 3 uin STRING openid(已加密) 4 roleid STRING 角色id(已加密) 5 rolelevel BIGINT 等级 6 Itemtype STRING 道具类型 7 Itemid STRING 道具ID 8 Count BIGINT 道具变动数量 9 Aftercount BIGINT 动作后道具剩余数量 10 Addorreduce BIGINT 增加 0/减少 1 11 Reason STRING 道具流动一级原因 12 SubReason STRING 道具流动二级原因 简单 eda训练集总账户数为74704，其中打金工作室账户10202，正常用户64502，正负样本分布不均匀。登入数据的离散数据统计信息如图1所示。 图1 登入数据的离散数据统计 登出数据字段和登入数据差不多，仅多了在线时长（onlinetime）特征，所以在后续的处理中，将二者合并成 operation 表放在一起使用，使得代码更简洁。rolename 全为空没有使用价值。从业务上讲，deviceid 是一个很重要的特征，根据设备id我们可以判断多账号共享设备的情况。但在数据集中，每天仅有一个deviceid，3.1号的 deviceid 全为6259A4950D8B0CA5，3.5号的 deviceid 全为71A1315F1949F262，失去了使用价值。 数据集给出了按天统计的帐号发言次数，整体分布如图2所示，具有很明显的长尾分布。分是否是工作室观察发言次数分布，如图3和图4所示，可以明显看出工作室的发言次数相较于正常用户较多，推测是在游戏中进行叫卖或者交易聊天。 图2 整体帐号发言次数 图3 工作室帐号发言次数 图4 正常用户帐号发言次数 用户创建角色数据，主要需要关注regchannel（注册渠道），总共有76种不同的注册渠道，主要渠道数量分布如图5所示。 图5 主要注册渠道数量分布 特征工程数据集给出了操作时间，可以提取出小时信息进行进一步统计。ip 特征能够很好的反映是否存在同一 ip 下的多账户行为。ip 特征比较细化，根据基本的网络知识，我们可以将其分段，得到隐藏的地域信息。所以在本方案中，构造了clientip_3 和 clientip_2，分别取 clientip 的前三段和前两段。logout 表给出了在线时长特征，类似的，我们可以将 login 表按时间排序，登入时间减去上一次的登出时间可以得到离线时长特征。 统计特征统计特征包括两方面，以uin为单位，分别采用不同的方法对数值特征和类别特征进行统计。对数值特征采取下列统计方法： 在线时长：sum，median，mean; 离线时长：mean，max 角色等级：mean，max; 战力：mean，max； 好友数量：mean，max； 用户操作时间（小时）：mean，min，max。 对下列类别特征进行count统计：platid, worldid, roleid, job, network, clientip, clientip_3, hour, regchannel。 ip 特征查资料可知，工作室往往会购买大量机器，利用自动化脚本进行打金，所以会出现多个账号共享 ip 的情况。我们计算每个 ip 下登录过多少个账户，再对某账户使用过的所有 ip 计算其登录过账户数的平均值，该值越大，说明该账户所处网段有大量账户登录，越有可能是工作室。 1234567891011121314151617for f in [&#39;clientip&#39;, &#39;clientip_3&#39;]: df_temp &#x3D; operation[[&#39;uin&#39;, f]] df_temp.drop_duplicates(inplace&#x3D;True) df_temp &#x3D; df_temp.groupby([f])[&#39;uin&#39;].nunique().reset_index() df_temp.columns &#x3D; [f, &#39;uin_count&#39;] df_temp2 &#x3D; operation[[&#39;uin&#39;, f]] df_temp2.drop_duplicates(inplace&#x3D;True) df_temp &#x3D; df_temp2.merge(df_temp, how&#x3D;&#39;left&#39;) df_temp &#x3D; df_temp.groupby([&#39;uin&#39;])[&#39;uin_count&#39;].agg(&#123; &#39;&#123;&#125;_uin_count_mean&#39;.format(f): &#39;mean&#39; &#125;).reset_index() df_feature &#x3D; df_feature.merge(df_temp, how&#x3D;&#39;left&#39;) del df_temp, df_temp2 gc.collect() 标签五折交叉统计特征在推荐任务中，经常使用点击率特征，即统计某个类别特征下点击商品的概率。同样作为二分类任务，我们也可以构造某个类别特征下是工作室的概率。该类特征利用了标签信息，容易出现标签泄露问题，所以在实际操作中，将训练集分为5份，每次使用4份做标签统计，得到的概率值给另外1份做特征。本方案构造了 chat_cnt 和 clientip_3 类别下的工作室概率。 embedding 特征ip 字段能帮助我们很好的判断是否存在账户聚集行为，假如我们能够将无法定量计算的 ip 地址转成模型能够识别的数字特征，将会帮助模型更好地利用 ip 信息。我们整理出每个账户在哪些 ip 操作过，将这些 ip 列表作为 sentence 输入到 word2vec 模型，将 ip 映射为向量，经常出现在上下文中的 ip 在 embedding 空间中也相近。最后将账户下所有 ip 的向量取平均作为新特征。效仿对 ip 特征的处理，我们还对 worldid 和 job 做了类似的操作。 流水特征决赛给出了角色的详细货币和物品流水信息(当天按时间顺序前300条记录)。结合业务考虑，工作室往往会进行大量的交易行为，实现物品或货币套利。所以对货币变化数，道具变动数量进行统计，包括求和，求平均值，求最大值和计数。货币变化可以详细到增加或减少，可以进一步在这两种情况下进行统计。此外数据集还给出了交易的理由，我们还可以统计每个理由下进行交易的货币数和物品数。货币类型，货币流动一级原因，道具流动一级原因，尤其是道具ID，这几个类别特征维度较大，分别进行统计会引入大量特征，而且数据普遍存在稀疏问题。以道具ID为例，ID 为342的物品只交易过一次，所以“物品342的交易数量”特征就会十分稀疏，只有一个账户下该特征有统计值。 最终，本方案总共使用704个特征，重要性比较高的特征如图6所示，可以看出账户的在线时长，离线时长特征很重要，因为工作室相较于正常用户会花费更多的时间进行游戏，追求利益的最大化。时段（hour）特征也比较重要，工作室不分白天黑夜进行游戏，而正常的用户不会在白天花费大量的时间进行游戏。加入流水特征后，Reason12下的货币交易尤为重要。 图6 特征重要性 伪标签在训练模型时，我们总希望输入的数据数据量尽可能大，数据标签尽可能分布均匀。在对测试集进行预测时，LightGBM 模型输出的是账户是打金工作室的概率，也可以理解为置信度，从图7可以看出，概率分布大多分布在两头，极有可能是和极有可能不是。我们完全可以将置信度较高的测试集数据作为标签为1的新训练集，从而达到扩增训练集且稍微平衡样本标签的目的。由于本次比赛没有榜单，所以这部分选择比较谨慎，只挑出概率值大于0.99的测试集数据添加到训练集，线下大概能提升0.006。 图7 概率值分布 模型利用 LightGBM 模型进行二分类任务，工作室账号标签设置为1，正常账号设置为0。采用五折交叉法，将3.1号的数据分为五份，每次使用4份训练，另外一份用来验证。由于样本标签分布不均衡，我们不能用0.5作为正负样本的分割点，而应该采用比例法。正样本大概占总样本的13.66%，所以将测试集的预测概率按从大到小排序，前13.66%挑选为工作室账号。在本地验证集上，精确率达到0.9700214132762313，召回率达到0.9324642227014311，比赛指标达到0.9603513111071851。 总结由于是高校赛，主办方大概考虑到学生党的机器性能限制，所以数据集并不是很大，比赛时间也比较紧凑，初赛12个小时，决赛三天，整体体验比较好。赛后阅读第一大佬的方案，特征工程思路基本相同，但大佬在流水统计方面做的更细致，并没有像我直接一股脑的对所有分类进行统计。此外还利用对抗训练进行特征筛选，删去在训练集和测试集上分布差异较大的特征。 通过role_itemflow，构造了主要物品类型（类型20025, 20035, 20036, 20028）的流水次数和物品变动数量，主要流水原因（原因84, 12, 85, 5）的流水次数。 参考资料 赛道第一名方案分享","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"反欺诈","slug":"反欺诈","permalink":"https://www.logicjake.xyz/tags/%E5%8F%8D%E6%AC%BA%E8%AF%88/"}]},{"title":"图灵联邦视频点击预测大赛-第三赛后总结","slug":"图灵联邦视频点击预测大赛-赛后总结","date":"2020-02-10T14:38:57.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"2020/02/10/图灵联邦视频点击预测大赛-赛后总结/","link":"","permalink":"https://www.logicjake.xyz/2020/02/10/%E5%9B%BE%E7%81%B5%E8%81%94%E9%82%A6%E8%A7%86%E9%A2%91%E7%82%B9%E5%87%BB%E9%A2%84%E6%B5%8B%E5%A4%A7%E8%B5%9B-%E8%B5%9B%E5%90%8E%E6%80%BB%E7%BB%93/","excerpt":"","text":"最终成绩：A, B榜第三。比赛地址：https://www.turingtopia.com/competitionnew/detail/e4880352b6ef4f9f8f28e8f98498dbc4/sketch代码地址：https://github.com/LogicJake/tuling-video-click-top3 赛题背景移动互联网的快速发展，催生了海量视频数据的产生，也为用户提供了类型丰富的视频数据类型。面对如何从海量视频数据类型中选择用户喜欢的类型的这一难题，作为一家拥有海量视频素材和用户行为的数据公司，希望通过用户行为数据，用户特征，以及视频特征，可以在充足数据基础上精准的推荐给用户喜欢的视频类型。 本次竞赛的目的是以用户的视频行为数据为基础，构建推荐模型，参赛队伍则需要搭建个性化推荐模型。希望参赛队伍能够挖掘数据背后丰富的内涵，为移动用户在合适的时间、合适的地点精准推荐用户感兴趣的内容，提高用户在数据集上的点击行为。 参赛者通过构建推荐模型，预测待测试数据中用户在对应的视频上是否会产生点击行为。 数据说明第一部分是用户在资讯全集上的移动端行为数据（D）。 训练数据包含了抽样出来的一定量用户在三天之内的移动端行为数据（D），评分数据是这些用户在之后一天对子集（P）的点击数据。参赛者要使用训练数据建立推荐模型，并输出用户在接下来一天点击行为的预测结果。 解决方案数据探索通过数据来源，我们能够更清楚地了解数据，了解业务背景。比赛数据采集于一款叫“亿刻看点”的 APP，此 APP 安卓独占，因为其某些流氓操作在苹果那边应该过不了审。这款 APP 以阅读赚钱（看应用市场评论提现很困难）为吸引点吸引用户使用，但有些迷之操作，在使用过程中会出现广告和下载弹窗，甚至退出这款 APP 后也能给我弹（如图1左）。APP 推荐的视频大多是影视剪辑，没有任何时效性，推荐策略也很单一：推荐与上次点击相似的视频。从图1右可以看到，当我点击了“那年花开”的视频片段后，刷新之后再推荐的就是该影视剧相关。 图1 APP 使用展示 从一个用户的角度，这款 APP 的用户体验不佳，首先广告弹出，误触下载让我无法忍受，所以不可能持续使用。即使有些人为了赚钱使用，也有极大几率为了奖励去点击视频，这样会产生大量不可信的操作数据。当一个人的行为数据不可信，对其未来的点击预估也就无从谈起了。在比赛中也有类似的体会，许多 ctr 预估中使用的常规特征收益很低甚至无效。 回归到数据本身，训练集总共给出了从2019.11.08 ~ 2019.11.10的三天数据，需要预测2019.11.11的用户点击行为。首先观察一下新老用户分布，下表列出了9，10，11三天昨日老用户占比。可以看出昨日老用户占比还是挺高，所以在后面的特征工程做了大量针对用户昨日行为的特征。按道理说这种体验感不好的 APP，在用户手机上存活时间不会太长，应该不会存在这么高的老用户比例，也不知道是奖励起到了作用还是数据提供方特意筛选出一部分活跃用户。 day 昨日老用户比例 9 0.6501773380623483 10 0.6388528497721992 11 0.7410879161463921 特征工程这一题数据量比较大，模型训练起来也比较慢，所以针对 lgb 模型准备了两套参数，一套学习率比较大用于快速迭代验证特征效果，训练一次大概在半小时左右，线上分数0.825。另外一套学习率为0.01的参数用于正式提交，运行一次大概需要15小时，线上分数0.8377。 穿越特征前面说过，这题常规特征收益很小，但由于数据给出了视频曝光时间（ts），所以可以借助其构造大量穿越特征，或者称作为视频点击后的模式特征。基本构造方法就是计算距离下一次视频曝光的时间差。这么做的原因也很好理解，假如一个人点击了某个视频，那么必然会观看一段时间，那么距离下一次视频的曝光就会久一点，ts 差值也较大。相反，连续两次视频的曝光时间间隔应该很小。距离上次视频的曝光时间差也是有效的，根据 APP 的推荐规则，在点击视频后下次推荐的也是相关视频，从而再次点击的可能性较大。我们构造了大量组合下的视频曝光时间间隔，曝光跨度也从1往后扩展，主要构造如下特征： deviceid 前x次曝光到当前的时间差 deviceid netmodel 前x次曝光到当前的时间差 deviceid 后x次曝光到当前的时间差 deviceid pos 后x次曝光到当前的时间差 deviceid netmodel 后x次曝光到当前的时间差 deviceid netmodel pos 后x次曝光到当前的时间差 deviceid lng_lat 后x次曝光到当前的时间差 deviceid lng_lat pos 后x次曝光到当前的时间差 除 ts 是个强特征之外，pos 也是强特。pos 取值范围为0 ～ 8，取值分布如图2左所示。抓包分析可知0 ～ 3这四个值对应首页推荐四个位置，但从图2右可以看到界面大小最多只够显示2个半，有曝光才有流量，所以才会导致 pos 取0，1，2的数据量相较于3较大。pos中的其他取值并没有找到在何处产生，猜想可能来自于相关推荐视频点击或者消息栏推送点击。进一步分析在不同 pos 下的点击率，如图3所示。可以看出同样是首页推荐位置，位置3虽然总数据不多，但点击率远高于其他首页位置。初步猜想大部分用户可能只打开看了一下，只看到首页屏幕大小展示的三个视频，并不会下滑查看更多推荐，所以才会出现0 ～ 2的数据量大，但点击率非常低的情况。对于深度使用用户，其要么出于完成任务拿奖励的目的还是正常使用，点击欲望相对更高，这部分用户在使用探索中会更多地看到 pos 3以上的视频。 图2 左：pos 取值分布；右：app 视频展示位 图3 各pos是否点击分布 效仿 ts 的穿越特征构造，我们又尝试构造了后x次视频曝光位置，同样效果也十分显著。仔细研究 APP 发现，假如你不是直接在首页点击播放视频，而是点击进入该视频的详情页，同样会触发视频观看，同时在视频详情页下会出现相关推荐视频，如图4红框所示。综上猜测，pos 中某些取值对应于相关推荐位，所以当下次视频曝光位置为上述相关推荐位，则表示当次视频一定是被用户点击观看的。 图4 相关推荐 通过学习上述两大类穿越特征，模型的效果已然十分明显，在快速迭代模型上，已经能够达到0.80114的分数。相较于最高分0.825，说明常规特征带来的收益只能提高0.024。穿越特征只有30+，但常规特征却有140左右，收益对比一目了然。工程实践中穿越特征自然无法使用，但作为一项数据挖掘比赛，当然是追求已有数据的最大化利用。 除了上述两大类穿越特征，还构造了下一次视频曝光的网络环境和基于经纬度的位置变化。 历史特征历史特征主要用过去一个时间单位的数据进行统计，然后作为当前时刻的特征。由于数据中昨日老用户占比较多，所以本方案大量构造了昨日数据统计特征。在这部分特征中大量涉及到各种点击率的构造，点击率使用到了标签数据，因此相较于全局统计点击率，构造昨日点击率避免了标签泄露和数据穿越问题。主要构造的点击统计特征如下： deviceid 点击次数，点击率 deviceid, hour 点击率 deviceid, netmodel 点击率 newsid 点击次数，点击率 next_pos 点击率 原数据给出了两个时间特征，一个是视频曝光时间 ts 和视频假如被点击时的点击时间 timestamp。二者的差值可以表示用户的反应时间，反应时间越短说明用户越喜欢该类视频。针对反应时间分别以 deviceid 和 newsid 为单位构造统计特征，构造方式包括：max，min，mean，std，median，kurt 和 quantile。反应时间从用户侧提取，所以针对 newsid 做统计误差较大，效果不明显，所以只保留了 std 统计。 除了以天为单位构造昨日数据特征，还以小时为单位构造上一小时的统计特征，尝试下来只有上一小时的 deviceid 点击次数和点击率有点效果。 count 统计这部分特征以不同时间单位进行 count 统计，时间单位包括：10分钟，小时，天，全局。不同于历史特征，这部分 count 统计往往会出现数据穿越问题。count 特征主要反应偏好属性，比如今天哪个视频曝光量大，用户倾向于在哪个时间，哪个网络环境下使用 APP。 12345678910111213141516cat_list = [['deviceid'], ['guid'], ['newsid'], ['deviceid', 'pos'], ['newsid', 'pos'], ['deviceid', 'guid', 'newsid'], ['deviceid', 'next_pos']]for f in tqdm(cat_list): df_feature['&#123;&#125;_day_count'.format('_'.join(f))] = df_feature.groupby(['day'] + f)['id'].transform('count') cat_list = [['deviceid'], ['guid'], ['deviceid', 'pos'], ['deviceid', 'netmodel']]for f in tqdm(cat_list): df_feature['&#123;&#125;_minute10_count'.format('_'.join(f))] = df_feature.groupby(['day', 'hour', 'minute10'] + f)['id'].transform('count') cat_list = [['deviceid', 'netmodel']]for f in tqdm(cat_list): df_feature['&#123;&#125;_hour_count'.format('_'.join(f))] = df_feature.groupby(['hourl'] + f)['id'].transform('count') cat_list = [['deviceid', 'group', 'pos']]for f in tqdm(cat_list): df_feature['&#123;&#125;_count'.format('_'.join(f))] = df_feature.groupby(f)['id'].transform('count') embedding 特征作为推荐中重要的一方，数据集给出的视频信息十分少，只有一列简单的视频 id，并没有视频分类之类的对推荐重要的属性信息，所以我们只能从视频被推荐给哪些用户下手。以视频为单位，找出被推荐的用户列表，每个列表作为句子喂到 Word2Vec 模型得到每个用户的 embedding 向量，用视频所有被推荐的用户的 embedding 向量平均值表示视频。得到 embedding 之后，就可以度量两个视频之间的相似度，所以也就产生了另外一种思路，当一个新视频被推荐给用户后，计算新视频与之前用户被推荐过（或者看过）的视频的平均相似度，平均相似度越大，用户点击的可能性越大。但在实际试验中效果不好，也不知道是不是 embedding 效果达不到的原因。 user 表的打开方式这一题特征构造主要还是围绕 train 表做的，user 表打开过多次都没啥效果。用户画像属性 tag 和 outertag 维度特别高，尝试过 PCA 降维和各种 embedding 方式都不行，最后还是采用的开源方法，对 tag 的分数做一些简单的统计。app 表的 applist 维度更高，也是各种尝试无效。 其他特征我们根据连续使用将用户的使用时间分段，曝光时间间隔少于3分钟的视为同一个使用时段。然后统计每个使用时段中上一次曝光时间差的 mean 和 std 特征。还统计了每个用户下一次曝光时间差的 mean，std，median，skew。针对曝光时间差的统计可以反应用户当前使用时段或者整体上的视频观看时长信息。 此外还构造了用户平均每小时被曝光多少视频，该视频是一小时内被曝光给用户的第几个视频，今日曝光给用户的视频量与昨日曝光给用户的视频量的差值，未来一小时用户在该网络环境下的视频曝光数量。 当然也有许多尝试后效果不好的操作，比如数据集给出了用户的设备信息：设备厂商，设备版本，可以效仿科大讯飞反欺诈中的操作，对其进行数据处理，但可能由于自身重要性就不大，数据处理也没啥用。借助百度地图 api 可以将数据集中的经纬度转化成具体的城市，也没啥效果。 模型这题主要有两种建模方式，一种是常规的五折交叉验证，在划分每折数据时，按 day 所占比例划分比以是否点击比例划分要好，可以让每折模型学习到三天的数据，做到时间上同分布。另一种按时间划分，线下用8，9两天的数据做训练集，10号数据做验证集，保留模型的最优迭代次数，然后线上模型用完整的三天数据预测测试集。对比来看，第二种验证集划分方式和线上更为相似，所以与线上分数 gap 较小，训练时间也更短，所以我们主要使用第二种建模方式。但第一种可以得到全数据集上的点击概率，可以作为新的特征加到数据集中，可以提升0.002-0.003。需要注意的是假如采用这种方式集成，五折模型的学习率不能太小，否则模型效果足够好导致概率值特征过强，使得时间划分模型直接拟合概率值，过早早停，无法学习其他特征。 总结个人感觉，受限于 APP 的用户体验，单纯的用户属性对预测作用不大，这也是为什么 app 表和 user 无法正确打开的原因。所以我们需要从用户的行为数据（也就是 train 表）构造特征，但行为数据中也有大量不可信操作。有些用户为了完成 APP 的任务拿奖励，会随意点击观看视频，这部分行为完全随机，无法从中正确提炼出用户的兴趣所在，也就很难去预测未来的点击行为。但用户观看视频后的模式是固定的，视频的播放时间是必定有的，所以 ts 的时间差是可信的。点击视频详情页出现的相关视频推荐也是必然存在的（这是我猜测的，可能不正确），所以下一次视频曝光的 pos 也是可信的。所以这一题常规特征干不过穿越特征，无法过于信任用户行为，只能信任机器行为。 参考资料比赛初期0.65开源天才儿童0.81高分开源user 表处理方式开源麻婆豆腐视频讲解","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"ctr预估","slug":"ctr预估","permalink":"https://www.logicjake.xyz/tags/ctr%E9%A2%84%E4%BC%B0/"}]},{"title":"国能日新第二届光伏功率预测赛-赛后总结","slug":"国能日新第二届光伏功率预测赛-赛后总结","date":"2020-02-07T13:45:57.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"2020/02/07/国能日新第二届光伏功率预测赛-赛后总结/","link":"","permalink":"https://www.logicjake.xyz/2020/02/07/%E5%9B%BD%E8%83%BD%E6%97%A5%E6%96%B0%E7%AC%AC%E4%BA%8C%E5%B1%8A%E5%85%89%E4%BC%8F%E5%8A%9F%E7%8E%87%E9%A2%84%E6%B5%8B%E8%B5%9B-%E8%B5%9B%E5%90%8E%E6%80%BB%E7%BB%93/","excerpt":"","text":"最终成绩：线上第六，无决赛。比赛地址：https://www.dcjingsai.com/common/cmpt/%E5%9B%BD%E8%83%BD%E6%97%A5%E6%96%B0%E7%AC%AC%E4%BA%8C%E5%B1%8A%E5%85%89%E4%BC%8F%E5%8A%9F%E7%8E%87%E9%A2%84%E6%B5%8B%E8%B5%9B_%E7%AB%9E%E8%B5%9B%E4%BF%A1%E6%81%AF.html 赛题背景光伏发电具有波动性和间歇性，大规模光伏电站的并网运行对电力系统的安全性和稳定造成较大的影响。对光伏电站输出功率的高精度预测，有助于调度部门统筹安排常规能源和光伏发电的协调配合，及时调整调度计划，合理安排电网运行方式。因此，本题旨在通过利用气象信息、历史数据，通过机器学习、人工智能方法，预测未来电站的发电功率，进一步为光伏发电功率提供准确的预测结果。 本场比赛不分A/B榜。 任务通过学习历史一段时间内的数值天气预测数据和对应的光伏发电功率训练模型，结合未来某时间点的数值天气预测数据，预测该时间点的光伏发电功率。 数据介绍原始数据集字段比较少，为每15分钟预测的气象数据，包括时间，辐照度，风速，风向，温度，压强，湿度，直辐射，散辐射，总辐射，实际辐照度，实际功率。除了实际辐照度为真实值，实际功率为任务需要预测外，其余都是预测的气象数据，这本身就是一个奇特的地方，实际功率的预测误差有一部分是由气象数据的预测误差引起的。主办方的奖励方案分为三挡，较高奖励需要指标达到0.11和0.1，然而即使比赛延期，最高分也只有0.13345。 数据包含10个电站的数据，每个电站的发电功率不一样，在计算指标 MAE 的时候，只考虑实际功率的值大于等于装机容量*3%的测量样本。 解决方案各个电站的分布差异性较大，模型在有些电站上拟合较好，有些较差，最好的电站 MAE 只有0.5左右，而差劲的电站能达到0.15。基于上述情况，采取各个站分开建模的方式，相较于统一建模效果提升显著。 数据预处理原始属性并没有缺失值，但异常值较多，除了官方提示的“电站9 2016/01/01 9:00 ~ 2017/03/21 23:45之间的实测数据可以删除；电站7 2018/03/01 00:00 ~ 2018/04/04 23:45之间的实测数据可以删除”，我们还额外进行了异常数据的删除。在训练集中，计算出每个站每天的平均功率，分别绘制散点图，会发现大量的离群点，我们根据离群点的范围加上手动调整，确定每日平均辐照度的合理范围，从而筛选出合理的训练数据。 查阅资料我们可以知道，对实际功率影响最大的气象因素为辐照度。在原始数据中，辐照度和实际功率的组合有很多重复值，大量的此类重复数据对模型的学习没有意义，使得模型学习偏向辐照度的影响，从而影响对其他特征的学习，所以我们删去了此类重复数据。但在特征工程中，有大量时序相关特征，为了保证时间上的连续性，我们在构造完特征后才删除重复数据。 123df_train = df_feature[df_feature['实际功率'].notnull()]df_test = df_feature[df_feature['实际功率'].isnull()]df_train.drop_duplicates(['辐照度', '实际功率'], inplace=True) 晚上没有太阳电站是不发电的，所以夜间数据也是无效的。所以我们将晚上7点之后，早上6点之前的数据进行了删除。 此外，原始气象数据成偏态分布，我们采用 np.log1p 将其正态化。 特征工程特征工程部分主要分为常规（统计为主）特征和业务特征。某个时间点的发电量不是孤立点，往往与前后的气象数据有关，因此在本方案中首先对气象数据进行平移，得到前15分钟，后15分钟，后30分钟，后45分钟的气象数据。除此以外还计算了一些窗口特征，如以当前时刻向前一个半小时时间段内风速, 辐照度, 温度, 压强, 湿度的 mean，std，sum，kurt 和 peak。窗口特征反映了站点局部气象变化情况，我们还可以放大时间跨度，以天，月，小时为单位有选择地统计气象数据，统计方法主要包括 median，max，min，std。气象数据都是数值型特征，我们可以利用分箱将其离散化，从而提高模型的稳定性和鲁棒性。之前我们说过辐照度对实际功率的影响最大，我们尝试刻画二者之前的关系，但假如直接用辐照度连续值来刻画，会受到异常值的影响，误差敏感。所以我们将辐照度进行等距分箱，采用五折统计框架统计每个分箱下对应的平均发电功率，用平均值降低异常值的影响。 气象数据对发电功率有很大影响，所以我们可以衡量气象数据趋势，比如风速，温度，湿度的趋势。在本方案中采用 LR 拟合一天的数据，用回归系数表示气象趋势。在基础气象数据的基础上，衍生构造出更多气象特征，比如：温差，湿度差，日温差。查阅资料可知：温度上升1℃，晶体硅太阳电池最大输出功率下降0.04%，开路电压下降0.04%(-2mv/℃)，短路电流上升0.04%。为了避免温度对发电量的影响，应该保持组件良好的通风条件。温度对太阳能板的发电功率影响很大，但在原始气象特征中，温度指示的是环境温度。但对于太阳能板，影响其发电功率的应该是板温，板温不仅和环境温度有关，也有风速有关，风越大，散热效果也就越好。进一步查阅资料发现板温与辐照度也有较大相关性，因此我们对辐照度，风速和环境温度进行综合计算得到板温。太阳入射角度和发电板倾斜角影响着太阳辐射到达太阳能板的实际辐照度，倾斜角度无法从已有数据获取，但太阳入射角（也叫天顶角）可以利用处于一年中的第几天，当地时间和经纬度位置计算得到，其中经纬度直接采用中国中部的大概位置。根据2018年DF光伏发电的冠军方案，距离周期内峰值的距离（dis2peak）也比较重要, 其反应一个测量点在一天，也就是一个周期中的位置，原因是每天中差不多的时间，太阳，温度等状态都差不多，发电量也就相似。 实际中，光伏发电量还与许多其他业务特征有关，比如和发电板电压，转换效率，但本次比赛只给了环境信息，无法构造。其他气象特征可能由于赛题本身提供的预测气象数据不准确，导致效果也不行。此外我们猜想天气会影响太阳能发电，比如下雪天积雪导致发电效能下降，所以尝试对气象数据进行 K-means 聚类，但效果也不好。 模型光伏发电往往随着时间周期变化，往小了有一天当中以小时为单位的周期变化，再往上一个月内以天为单位的周期变化。所以在五折交叉验证中，采用 StratifiedKFold 对 day+hour 划分，保证每折模型能学习各阶段数据特性，不会出现有偏学习，换句话说每个模型的训练数据是同分布的。模型方面我们使用了 lgb + xgb，最后使用 stacking 的方式融合。 参考资料Data Fountain光伏发电量预测 Top1 开源分享XGBoost+LightGBM+LSTM:一次机器学习比赛中的高分模型方案浅谈影响光伏发电量减少的主要因素气象要素对太阳能电池板温度的影响","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"光伏功率","slug":"光伏功率","permalink":"https://www.logicjake.xyz/tags/%E5%85%89%E4%BC%8F%E5%8A%9F%E7%8E%87/"}]},{"title":"2019科大讯飞移动广告反欺诈算法挑战赛算法-亚军赛后总结","slug":"移动广告反欺诈算法挑战赛算法-赛后总结","date":"2020-02-06T13:06:34.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"2020/02/06/移动广告反欺诈算法挑战赛算法-赛后总结/","link":"","permalink":"https://www.logicjake.xyz/2020/02/06/%E7%A7%BB%E5%8A%A8%E5%B9%BF%E5%91%8A%E5%8F%8D%E6%AC%BA%E8%AF%88%E7%AE%97%E6%B3%95%E6%8C%91%E6%88%98%E8%B5%9B%E7%AE%97%E6%B3%95-%E8%B5%9B%E5%90%8E%E6%80%BB%E7%BB%93/","excerpt":"最终成绩：复赛第二，决赛亚军。比赛地址：http://challenge.xfyun.cn/2019/gamedetail?type=detail/mobileAD 赛题背景讯飞AI营销云基于深耕多年的人工智能和大数据技术，赋予营销智慧创新的大脑，以健全的产品矩阵和全方位的服务，帮助广告主用AI+大数据实现营销效能的全面提升，打造数字营销新生态。 广告欺诈是数字营销面临的一个重大挑战，随着基础技术的成熟化（篡改设备信息、IPv4服务化、黑卡、接码平台等），广告欺诈呈现出规模化、集团化的趋势，其作弊手段主要包括机器人、模拟器、群控、肉鸡/后门、众包等。广告欺诈不断蚕食着营销生态，反欺诈成为数字营销领域亟待突破的关键问题。","text":"最终成绩：复赛第二，决赛亚军。比赛地址：http://challenge.xfyun.cn/2019/gamedetail?type=detail/mobileAD 赛题背景讯飞AI营销云基于深耕多年的人工智能和大数据技术，赋予营销智慧创新的大脑，以健全的产品矩阵和全方位的服务，帮助广告主用AI+大数据实现营销效能的全面提升，打造数字营销新生态。 广告欺诈是数字营销面临的一个重大挑战，随着基础技术的成熟化（篡改设备信息、IPv4服务化、黑卡、接码平台等），广告欺诈呈现出规模化、集团化的趋势，其作弊手段主要包括机器人、模拟器、群控、肉鸡/后门、众包等。广告欺诈不断蚕食着营销生态，反欺诈成为数字营销领域亟待突破的关键问题。 数据说明本次比赛为参赛选手提供了5类数据：基本数据、媒体信息、时间、IP信息和设备信息。基本数据提供了广告请求会话sid，以及“是否作弊”的标识。媒体信息、时间、IP信息和设备信息等4类数据，提供了对作弊预估可能有帮助的辅助信息。 出于数据安全保证的考虑，所有数据均为脱敏处理后的数据。数据集提供了若干天的样本，最后一天数据用于预测，不提供“是否作弊”标识，其余日期的数据作为训练数据。 解决方案数据预处理这一题数据预处理占了很多篇幅，主要包括两部分：对无意义字符或乱码的处理；基于业务需要的数据处理。在我们的方案中，着重对操作系统（osv），手机制造厂商（make）和机型（model）进行处理。有些 osv 包含无意义的字段，比如“??????????????????????????؉*o\\x18”或“..þ\\x06&lt;þ\\x1f&lt;þf&lt;þ`&lt;þ]&gt;þܾþ\\x08?þ2?þz?þ|?þ\\u07ffþ?þ”，我们直接将其全部替换成-1。osv 主要由“.”分隔的数字组成，如7.1.9的形式，但在实际数据中包含一些由其他字符甚至字母分隔的osv，将这些分隔符替换成“.”，保证统一性。 make 最主要的问题是同一厂商下的中英文名称共存，比如“华为”和“huawei”没有必要认为其是两个不同的厂商（滑稽），所以统一将中文名称转成小写英文名称。此外对于一些过于精细化的分类进行粗放，比如“荣耀”归到“huawei”，“m1”，“m2”，“m3”全都归到“xiaomi”。 model 大部分数据格式都是“厂商 具体机型”，但很多数据都是采用厂商自己独有的编号，比如 OPPO 就有“PACM00”，“PBAM00”…因为 OPPO 这类数据较多，所以特意对其进行标准格式转换。 123456789data['model'].replace('PACM00',\"OPPO R15\", inplace=True)data['model'].replace('PBAM00',\"OPPO A5\", inplace=True)data['model'].replace('PBEM00',\"OPPO R17\", inplace=True)data['model'].replace('PADM00',\"OPPO A3\", inplace=True)data['model'].replace('PBBM00',\"OPPO A7\", inplace=True)data['model'].replace('PAAM00',\"OPPO R15_1\", inplace=True)data['model'].replace('PACT00',\"OPPO R15_2\", inplace=True)data['model'].replace('PABT00',\"OPPO A5_1\", inplace=True)data['model'].replace('PBCM10',\"OPPO R15x\", inplace=True) 以上三列数据还存在一个共有问题：存在一些 url 编码字符，将其转换成对应的字符。另外对于’h’, ‘w’, ‘ppi’这些数值属性的缺失值，采用同一设备生产厂商下该属性的平均值填充。 特征工程这一题有个明显的特点就是类别特征特别多，众所周知 catboost 擅长处理类别特征，所以直接将原始特征喂到 catboost，相对于其他模型效果非常好。在我的理解中，判断欺诈行为，其实就是通过多因素的累加来逐步确认欺诈的可能性，比如说来自某个城市的点击欺诈行为可能性为20%，假如其同时又是使用的某个欺诈行为频繁的机型，则欺诈可能性就又会提高。所以在这题中，对类别特征进行大量的交叉组合是有效的，这也是为什么 catboost 的效果要好，catboost 会自动进行类别特征的组合。在比赛交流群，有人问既然 catboost 已经能够完成特征组合，那么特征工程还有必要做大量的组合特征吗？我认为是必要的，这个道理其实和“深度模型理论上能够完成特征的高阶组合，但在比赛中人工特征工程还是存在的”一样，都是为了在源头上降低拟合的难度，提升性能的上限。 我们还可以根据 count 和 nunique 来挖掘欺诈行为，比如 imei 是手机串号，一部手机对应一个唯一的 imei。欺诈团伙为了躲避制裁往往会刷机，从而改变 model 值，比如将原本的华为手机刷成小米。针对以上行为，我们可以统计 imei 有多少个不同的 model ，通过这个特征判断这是一个存在刷机行为的设备。类似的，针对设备挂代理可以统计 imei 有多少个不同的 city。我们也可以构造其他特征来寻找欺诈行为，比如欺诈团伙会同时控制大量的手机进行广告点击，因此会大量采购某些价廉的机型，对应的这些机型会点击大量且不同的广告。这都是可以通过 count 和 nunique 特征和正常点击行为分开的，因为正常的用户不会去点击那么多广告。 除了 count 和 nunique 统计特征，还可以模仿点击率构造欺诈率特征，比如某个 model 或者 city，欺诈次数占总次数的比率。构造欺诈率特征的出发点来自赛题背景中所说的“广告欺诈呈现出规模化、集团化的趋势”，规模化，集团化的显著特点就是欺诈行为集中在某个城市，大量使用某个机型进行欺诈。此外，赛题中的 ip 信息也可以用来发现规模化，集团化行为，ip 能给出很多有用信息，比如 ip 能反应运营商，设备所在网段，从而反应这部分设备是否处在一个大量且集中的环境下，如下图所示。所以可以对 ip 按’.‘进行分段，取前一个，前两个，前三个作为新的网段类别特征，相较于 city 进一步细化了集团化，规模化概念。 手机集群（图片来自网络） 模型这题毫无疑问采用 catboost 比较好，但巨大的数据量加上大量的类别特征导致十分吃内存，模型训练完成回退到最优迭代轮次的时候经常内存溢出。初赛的时候采用 lgb 和 catboost 进行投票，到复赛直接就 catboost 但模型了。最后复赛 A 榜98.26379，B 榜98.24133，还是很稳定的。 总结观看现场前三的答辩，总体思路上大家都差不多。第三名队伍小兔子乖乖采用了“伪标签”，将测试集当中预测概率值大于0.999的作为正样本，小于0.001的作为负样本加入到训练集重新训练，提高了训练集的数据量，这一做法使得模型效果提高了0.0035左右。第一名直接是一个公司组的队伍，其主要亮点就在于通过部分有规则碰撞的方式还原 MD5 加密了的 imei，然后对还原后的 imei 进行截断，具体流程见下图。 imei 逆向1 imei 逆向2 参考资料 DNN可以进行高阶特征交互，为什么Wide&amp;Deep和DeepFM等模型仍然需要显式构造Wide部分？ 现场决赛录播","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"反欺诈","slug":"反欺诈","permalink":"https://www.logicjake.xyz/tags/%E5%8F%8D%E6%AC%BA%E8%AF%88/"}]},{"title":"“添翼”杯人工智能创新应用大赛-智慧教育赛道-三等奖赛后总结","slug":"tianyicup-education","date":"2020-01-01T00:00:00.000Z","updated":"2021-07-03T16:10:11.529Z","comments":true,"path":"2020/01/01/tianyicup-education/","link":"","permalink":"https://www.logicjake.xyz/2020/01/01/tianyicup-education/","excerpt":"前言这是我接触数据挖掘后的第一个与之有关的比赛，很标准的表格题，业务背景也不复杂，自我感觉非常适合像我这样的新人去完整地了解，熟悉数据挖掘比赛的整体流程。我也是在这次比赛中慢慢了解特征工程的重要性，也了解并应用了一些基础的特征构造方法，也第一次听说并使用（还没探索其背后的原理）那些个 boosting 模型：xgboost，catboost，lightgbm。","text":"前言这是我接触数据挖掘后的第一个与之有关的比赛，很标准的表格题，业务背景也不复杂，自我感觉非常适合像我这样的新人去完整地了解，熟悉数据挖掘比赛的整体流程。我也是在这次比赛中慢慢了解特征工程的重要性，也了解并应用了一些基础的特征构造方法，也第一次听说并使用（还没探索其背后的原理）那些个 boosting 模型：xgboost，catboost，lightgbm。 在比赛中也认识了队友，油菜花和勇哥，团队带来的学习增益和激励增益是庞大的，队友积累的经验和知识也是一笔巨大的财富。 最终成绩：复赛第三，决赛三等奖。 代码地址：https://github.com/LogicJake/tianyicup-education 赛题背景随着人工智能(AI)的发展，“AI＋教育”“智慧课堂”等名词逐渐出现在大众视野，越来越多的学校将人工智能助手融入课堂，当下中国正逐步进入“智慧教育”时代。在传统课堂中，由于时间和精力的限制，老师和家长无法兼顾学生的学习状态和学业进展，不会关注大量对于学生能反应其真实问题和情况的数据。 智慧教育通过将传统教育行业的场景和当下最新的人工智能算法紧密结合，深度挖掘学生在各个知识点上的历史答题表现数据，最终预测学生在考试中的分数表现。 赛题描述请参赛选手，利用比赛对应训练集提供的学生信息、考试知识点信息、考试总得分信息等建立模型，预测测试集中学生在指定考试中的成绩总分，预测目标如下： 初赛：利用初中最后一年的相关考试和考点信息，预测初中最后一学期倒数第二、第三次考试的成绩。复赛：利用初中 4 年中的相关考试和考点信息，预测初中最后一学期最后一次考试的的成绩。 数据下载初赛数据下载（右键保存下载），复赛数据不可下载。 解决方案简单的数据分析 课程类别 课程名称 知识点数量 course_class1 course1 368 course2 367 course3 262 course_class2 course4 219 course5 335 course6 146 course7 316 course8 223 课程数据包括课程类别，课程名称和所包含的知识点。课程类别只有两类，大概率就是理科和文科的区别。主要难点在于课程的知识点维度非常大，课程考试虽然每次只包含少量的知识点，但是为了向量空间统一，假如采用01向量表示考试的知识点分布情况，该向量将是巨维且十分稀疏的。 成绩分布 该数据集还有个好处就是数据完整，不存在数据缺失或异常值的情况，除了标签数据‘成绩’出现了0分。从上图成绩分布可以看到，大部分成绩都在60分以上，60以下的就很稀疏。我们将0分看做异常值，可以认为是由学生缺考或者作弊被取消成绩导致的，这部分0分数据不能代表学生正常的考试水平且模型也学习不到这种无规律情况，所以将这部分数据进行剔除。由上面成绩分布可以知道，模型的预测也大概率集中在60以上，所以对于0分样本，计算出来的MSE指标非常大。所以这一题线上线下MSE分数差距比较大，可以确定线上测试集也包含了0分样本。当时曾经和队友讨论能否单独建模预测0分的出现，但效果很差，毕竟学生缺考和作弊行为也没法从给出的数据中推演出来（笑）。 特征工程基础特征基础的特征工程比较简单，就是做一下统计特征，比如考试次序，考试的知识点数量和知识点跨度。将知识点映射为其所属段落和类别，又可以构造数量和跨度特征。每个知识点有难度属性，每门考试的知识点占比乘以对于难度，得到衡量考试整体难度的特征。在数据分析中提到，知识点维度巨大且稀疏，假如使用原始01向量代表知识点的分布，会大大降低模型的效率。所以我们使用经典的PCA进行降维，在实际操作中，对考试的知识点占比，知识点种类和段落占比分别降维到60维。当然我们也尝试过使用 deepwalk 之类的 embedding 方法进行降维，但效果不行，在决赛答辩阶段，听其他队伍介绍也使用了 deepwalk， 但当评委问起 embedding 效果时，也回答效果不好。此外还简单使用了交叉特征，如知识点数量和考试难度拼接交叉。 相似考试特征在构造特征的时候，我们认为一个学生面对一门新考试，其成绩应该与和这次考试相似的考试成绩相近。所以我们找出每门考试最相似的三门考试，然后对学生三门考试成绩与考试相似度进行加权求和，得到预估的本次考试成绩，作为一个伪标签。在定义相似时，我们采用了三个指标：知识点分布相似，知识点段落分布相似，知识点类别分布相似。 相似考试特征构造示意图 五折交叉标签统计特征在接触这个比赛之前，我一直认为特征是特征，标签是标签，标签信息是无法使用或丢给模型学习的。这一点本质没错，因为直接使用标签进行学习，必然造成数据泄露，得到一个毫无意义的模型。但只要稍加变通，就能充分使用标签信息。换句话说，我们在评估一个学生的成绩好坏时，常常会说这个学生的平均成绩如何如何，这个时候不也是用了标签信息吗？甚至我们可以因此推断，学生的考试平均成绩，因为是一个强特征，它对预测学生考试成绩时十分重要。因此引入了特征工程中一个比较重要的构造方式，target encode，其在 ctr 类型的比赛中使用广泛。刚刚我们说过，一方面平均成绩很重要，但另一方面引入标签信息会泄露。泄露的本质就是在为样本构造特征的时候，使用了本样本的标签信息，那么为了避免泄露，我们在 target encode 时，只要避免使用本次样本的标签信息就可以了，常见的做法为五折交叉统计，将历史考试成绩数据分为五折，每次用4折构造特征，给训练集中的另外1折。 五折交叉统计(引用自鱼佬知乎专栏) 以此类推，我们不仅可以构造成绩平均值，还可以借助五折框架构造更多关于成绩的统计特征。将构造过程分为对象和统计方法两部分：对象: 学生所有考试成绩 学生所有考试排名 学生考试成绩 / 本次考试平均分 学生在所有考试上表现出来的抗压能力 学生在某门课考试上表现出来的抗压能力 某门课程的所有考试成绩 学生在某门课程上的考试成绩 学生在某门课程上某次考试成绩 / 某次考试平均成绩 某性别学生在某门课程上所有考试成绩 统计方法: 最大值(max) 最小值(min) 平均值(mean) 标准差(std) 中值(median) 变异系数(cv) 时序特征学生最近几次的考试成绩可以反映一个学生最近的学习状态，所以我们可以用最近几次的考试成绩构造时序特征。差分，窗口是时序常用的方法。 最近三次考试成绩 mean std 最近三次考试排名 mean std 最近三次考试成绩差值的平均 对最近三次考试成绩平均分做窗口为8的平均 对最近三次考试成绩平均分差值做窗口为3的平均 最近三次考试成绩 “嫁接”特征“嫁接”学习源于这样一个问题：初赛复赛数据分布不一致。在IJCAL2018比赛中,大家发现前六天和最后一天数据分布不同，大部分人于是用同分布的第七天上半天的数据预测下半天，而植物大佬在这个基础用前六天训练了一个模型，预测第七天得到的 probability 作为第七天模型的 feature ，再用第七天上半天的数据预测下半天，使最后的分数遥遥领先，轻松得到 solo 冠军。套用到这个比赛，复赛利用初中 4 年中的相关考试和考点信息，预测初中最后一学期最后一次考试的的成绩。我们可以认为前三年的考试和最后一年的考试是不同分布，而要预测的最后一次考试和最后一年的考试是同分布的，所以我们利用前三年数据构造部分特征，预测最后一年的考试成绩。利用全量数据构造的特征加上预测的最后一年的考试成绩预测最后一次考试成绩。 模型本方案采用五折交叉验证框架，分别训练 lightgbm 和 xgboost 模型，将两个模型的预测分数根据线上模型得分加权融合。模型分数见下表。 模型 Public分数 xgboost 7.289815178 lightgbm 7.288464145 融合 7.286831301 总结赛后拜读了第一大佬的开源。第一名方案对特征分别建模训练，总共分为三部分模型：第一部分模型将成绩预测问题视为回归问题进行分析，第二部分模型将成绩预测问题视为一种短期时间序列预测的问题，第三部分模型对成绩、试卷考点分布、学生考点掌握整体进行数学抽象。在第一部分模型当中，处理考试的知识点特征的时候，采用 NMF 算法进行降维处理。最让我感兴趣的是第三部分模型，在我的方案中，其实对知识点处理的不是很好，无法构造出学生对知识点的掌握情况。第一名方案采用这样一种假设：试卷-考点分布矩阵（exam）乘以学生-考点掌握矩阵（stu）就能得到试卷-学生成绩矩阵（score）。exam 矩阵和 score 矩阵都是已知，所以直接采用优化算法解得 stu 矩阵即可。形式化表示如下： score : m * n (exam_number * student_numbers) 试卷-学生成绩矩阵 exam : m * s(exam_number * course_section) 试卷-考点分布矩阵 stu : s * n (course_section * student_numbers) 学生-考点掌握矩阵 其中，m 为各科考试次数，n 为学生数目 s 为各科考点数，目标函数为 Argmin (Σ(score - exam * stu)^2)。最后通过学生-考点掌握矩阵乘以要预测的考试知识点分布矩阵，从而得到待预测考试的预测成绩。从模型效果来看，该部分方案十分优秀，线上得分7.33，令人佩服。 参考资料 2019腾讯广告算法大赛方案分享（初赛冠军） 结构化数据的迁移学习：嫁接学习 第一名方案","categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"表格数据","slug":"表格数据","permalink":"https://www.logicjake.xyz/tags/%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE/"},{"name":"教育","slug":"教育","permalink":"https://www.logicjake.xyz/tags/%E6%95%99%E8%82%B2/"}]},{"title":"A*算法理解及代码实现","slug":"A_star","date":"2019-12-28T12:12:07.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"2019/12/28/A_star/","link":"","permalink":"https://www.logicjake.xyz/2019/12/28/A_star/","excerpt":"A*寻路算法要解决的问题就是在有障碍物的情况下，如何快速找到一条到达目的节点的最短路径。 把问题抽象成以下场景：在一个由M×N的方块组成的区域中，绿色代表起始点，蓝色代表无法越过的障碍物，红色代表终点。需要注意的是，我们在寻路的时候无法越过“墙角”，对照到下图就是不能走红色路线，必须走蓝色路线。这是因为在抽象场景下，移动物体是无体积所以可以直接沿着红线穿过去，但在实际情况下，比如无人车寻路，考虑到体积因素是无法行进红色路线的，所以在建模的时候需要加上这样的约束条件。当然在不涉及到墙角的情况下是可以走斜线的，这是毋庸置疑的。","text":"A*寻路算法要解决的问题就是在有障碍物的情况下，如何快速找到一条到达目的节点的最短路径。 把问题抽象成以下场景：在一个由M×N的方块组成的区域中，绿色代表起始点，蓝色代表无法越过的障碍物，红色代表终点。需要注意的是，我们在寻路的时候无法越过“墙角”，对照到下图就是不能走红色路线，必须走蓝色路线。这是因为在抽象场景下，移动物体是无体积所以可以直接沿着红线穿过去，但在实际情况下，比如无人车寻路，考虑到体积因素是无法行进红色路线的，所以在建模的时候需要加上这样的约束条件。当然在不涉及到墙角的情况下是可以走斜线的，这是毋庸置疑的。 不能直接越过墙角 算法流程首先定义open list和close list，open list存放已知但还没有探索过的区块，close list存放已经探索过的区块。 最短路径肯定涉及到距离度量，在A*算法中距离分为两个部分：G 和H，总距离F=G + H。 G等于从起点移动到指定方格的移动代价。在本例中，相邻节点间，横向和纵向的移动代价为 10 ，对角线的移动代价为 14 （10×根号2的近似）。为了方便计算和寻路，我们为每个节点设置一个父节点。父节点可以这样理解，在目前已知条件下，存在一条从起点到当前指定方格的最优路径，而父亲节点就是这条路径上的指定方格的上一个节点，计算当前方格的 G 值的方法就是找出其父亲的 G 值，然后按在父亲节点直线方向还是斜线方向加上 10 或 14。 H为从当前节点到终点的估计距离，是对剩余距离的估算值，而不是实际值。它是一种理想值，忽略了障碍物的影响。在本例中使用曼哈顿距离（街区距离）来度量剩余距离。 整个算法流程为： 把起点加入open list，重复以下流程 如果open list为空，寻路失败，找不到到达终点的路径。遍历 open list ，查找 F 值最小的节点，把它作为当前要处理的节点。 把这个节点移到 close list 对当前方格的 8 个相邻方格的每一个方格 如果它是不可抵达的或者它在 close list 中，忽略。 如果它不在 open list 中，把它加入 open list ，并且把当前方格设置为它的父亲，计算该方格的 F ， G 和 H 值。 如果它已经在 open list 中，检查通过当前方格到达该方格是否代价更小，即G值更小。如果是这样，把它的父亲设置为当前方格，并重新计算它的 G 和 F 值。 如果终点加入到了open list中，此时路径已经找到，从终点开始，每个方格沿着父节点移动直至起点，这就是最优路径。 由算法可以看出通过总距离F选出当前处理节点，通过G来更新路径（改变节点的父节点就是改变了路径）。 另外需要注意：在寻找F值最小的时候可能会出现不止一个节点的情况，此时处于节省寻路时间的考虑，选择最后放入open list的节点。因为最后放入open list的节点是上一个处理节点的邻居节点，从而保证寻路时的连贯性，不会出现在寻路过程中突然跳到另外的地方重新开辟一条新路径。 流程解释解释流程前，先说明图例： 绿色填充方块：起点 蓝色填充方块：障碍 红色填充方块：终点 绿色边的方块：open list中的方块 黄色边框方块：close list中的方块 方块中白色箭头指向父亲节点 方块中左上角数字代表F值，左下角G值，右下角H值* 第1次搜索 开始搜索从open list中取出起始点，将起点加入close list。起点周围8个方格都可到达所以都加入到open list中，设置父节点为起点，并计算各自的F，G，H值。结果如上图所示。 继续搜索从open list中找出F值最小的方格，起点右边的方格F值为40最小，暂且称该节点为A。将A从open list剔除，加入到close list。A右边为障碍物，忽略；其余方向的方格都已经在open list中且加入A并没有减小他们的G值，所以维持原样不变。 结果如下图所示，可见起点右边的方格加上了黄色框，代表进入close list，其余不变。 第2次搜索结果 重复以上步骤，值得注意的是在第5次搜索，随着起点正下方方格（称其为B）加入到close list，处于B下方的方格（称其为C）因为B的加入，起点到C的距离缩短到80，所以C的父节点跟新为B，并相应跟新F，G，H的值。 第4次搜索结果 第5次搜索结果 不断重复上述步骤，最后终点被加入到open list中，从终点开始，每个方格沿着父节点移动直至起点，就是最优路径。 最终结果 全部过程 代码实现https://github.com/LogicJake/A-star-search","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://www.logicjake.xyz/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"hello world","slug":"hello-world","date":"2019-12-27T22:10:07.000Z","updated":"2021-07-03T16:10:11.497Z","comments":true,"path":"2019/12/27/hello-world/","link":"","permalink":"https://www.logicjake.xyz/2019/12/27/hello-world/","excerpt":"","text":"之前博客的服务器不小心被我格式化了，还是搞个静态的托管在 github page 省心。","categories":[],"tags":[]}],"categories":[{"name":"比赛总结","slug":"比赛总结","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/"},{"name":"数据挖掘","slug":"比赛总结/数据挖掘","permalink":"https://www.logicjake.xyz/categories/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"https://www.logicjake.xyz/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"召回","slug":"召回","permalink":"https://www.logicjake.xyz/tags/%E5%8F%AC%E5%9B%9E/"},{"name":"排序","slug":"排序","permalink":"https://www.logicjake.xyz/tags/%E6%8E%92%E5%BA%8F/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.logicjake.xyz/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://www.logicjake.xyz/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"表格数据","slug":"表格数据","permalink":"https://www.logicjake.xyz/tags/%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE/"},{"name":"价格预测","slug":"价格预测","permalink":"https://www.logicjake.xyz/tags/%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B/"},{"name":"反欺诈","slug":"反欺诈","permalink":"https://www.logicjake.xyz/tags/%E5%8F%8D%E6%AC%BA%E8%AF%88/"},{"name":"ctr预估","slug":"ctr预估","permalink":"https://www.logicjake.xyz/tags/ctr%E9%A2%84%E4%BC%B0/"},{"name":"光伏功率","slug":"光伏功率","permalink":"https://www.logicjake.xyz/tags/%E5%85%89%E4%BC%8F%E5%8A%9F%E7%8E%87/"},{"name":"教育","slug":"教育","permalink":"https://www.logicjake.xyz/tags/%E6%95%99%E8%82%B2/"},{"name":"算法","slug":"算法","permalink":"https://www.logicjake.xyz/tags/%E7%AE%97%E6%B3%95/"}]}